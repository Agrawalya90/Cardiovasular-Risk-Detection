import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
df=pd.read_csv(r'A:\Disease py/DeepDiag.csv')
sampled_df = df.sample(n=50000, random_state=42)
pd.set_option('display.max_columns', None)
print(sampled_df.head())
print(sampled_df.info())
print(sampled_df.isnull().sum())
print(sampled_df.describe().transpose()) 
print(sampled_df.duplicated().sum())
sampled_df=sampled_df.drop_duplicates()
sns.countplot(data=sampled_df, x='Heart_Disease')
plt.title('Distribution of Heart Disease')
plt.show()
sns.histplot(data=df, x='BMI', kde=True)
plt.title('Distribution of BMI')
plt.show()

heart_disease_counts = sampled_df['Heart_Disease'].value_counts()
print("\nHeart Disease Count:")
print(heart_disease_counts)

label_encoder=LabelEncoder()
for column in sampled_df.select_dtypes(include='object'):
    sampled_df[column] = label_encoder.fit_transform(sampled_df[column])
print(sampled_df['Heart_Disease'].value_counts())
print(sampled_df.head())

X = sampled_df.drop("Heart_Disease", axis = 1)
y = sampled_df['Heart_Disease']
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X, y)
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)
logistic_predictions = logistic_model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, logistic_predictions)
print(f"Logistic Regression Accuracy: {accuracy:.2f}")
print("Logistic Regression Classification Report:")
print(classification_report(y_test, logistic_predictions))



dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, dt_predictions)
print(f"Decision Tree Classifier Accuracy: {accuracy:.2f}")
print("Decision Tree Classifier Classification Report:")
print(classification_report(y_test, dt_predictions))


#Top 10 Features
imp_df = pd.DataFrame({
    "Feature Name": X_train.columns,
    "Importance": dt_model.feature_importances_
})
fi = imp_df.sort_values(by="Importance", ascending=False)

fi2 = fi.head(10)
plt.figure(figsize=(10,8))
sns.barplot(data=fi2, x='Importance', y='Feature Name')
plt.title('Top 10 Feature Importance Each Attributes (Decision Tree)', fontsize=18)
plt.xlabel ('Importance', fontsize=16)
plt.ylabel ('Feature Name', fontsize=16)
plt.show()


rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, rf_predictions)
print(f"Random Forest Classifier Accuracy: {accuracy:.2f}")
print("Random Forest Classifier Classification Report:")
print(classification_report(y_test, rf_predictions))

imp_df = pd.DataFrame({
    "Feature Name": X_train.columns,
    "Importance": rf_model.feature_importances_
})
fi = imp_df.sort_values(by="Importance", ascending=False)

fi2 = fi.head(10)
plt.figure(figsize=(10,8))
sns.barplot(data=fi2, x='Importance', y='Feature Name')
plt.title('Top 10 Feature Importance Each Attributes (Random Forest)', fontsize=18)
plt.xlabel ('Importance', fontsize=16)
plt.ylabel ('Feature Name', fontsize=16)
plt.show()
